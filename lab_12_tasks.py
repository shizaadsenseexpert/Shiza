# -*- coding: utf-8 -*-
"""lab 12 tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I1yCC5i8LhrKakuUMXVfD7pyMEdqswkf
"""

#!/bin/bash
!pip install kaggle nltk pandas

import os
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize

# Download NLTK data for tokenization
try:
    nltk.download('punkt')        # Main tokenizer resource
    nltk.download('punkt_tab')    # Additional resource to avoid 'punkt_tab' error
except Exception as e:
    print("Error downloading NLTK resources:", e)

# Step 1: Set up Kaggle API credentials
os.environ['KAGGLE_USERNAME'] = "sheezazahid"
os.environ['KAGGLE_KEY'] = "a00300cd353f3cb41559c53599a22955"

# Step 2: Download the IMDB dataset using Kaggle API
!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

# Step 3: Unzip the dataset
!unzip -o imdb-dataset-of-50k-movie-reviews.zip

# Step 4: Load the IMDB dataset
temp_df = pd.read_csv('/content/IMDB Dataset.csv')

# Step 5: Define a function for word-based tokenization
def tokenize_reviews(reviews):
    """
    Tokenizes each review into words.

    :param reviews: A pandas Series or list containing textual reviews.
    :return: A list of tokenized reviews (list of word tokens per review).
    """
    try:
        return [word_tokenize(review) for review in reviews]
    except Exception as e:
        print("Error during tokenization:", e)
        return None

# Step 6: Apply tokenization to the reviews
try:
    # Tokenize the reviews
    temp_df['tokenized_review'] = tokenize_reviews(temp_df['review'])

    # Save the updated dataset with tokenized reviews
    temp_df.to_csv('/content/IMDB_Dataset_with_Tokens.csv', index=False)

    # Display an example of the original and tokenized review
    print(f"Original Review: {temp_df['review'][0]}")
    print(f"Tokenized Review: {temp_df['tokenized_review'][0]}")
    print("Tokenized dataset saved as '/content/IMDB_Dataset_with_Tokens.csv'")

except Exception as e:
    print(f"An error occurred: {e}")

#!/bin/bash
!pip install kaggle nltk pandas scikit-learn

import os
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Step 1: Ensure NLTK data is downloaded
try:
    nltk.download('punkt')
    nltk.download('punkt_tab')  # Added to resolve the punkt_tab error
    nltk.download('stopwords')
    nltk.download('wordnet')  # Needed for lemmatization
except Exception as e:
    print("Error downloading NLTK resources:", e)

# Step 2: Set up Kaggle API credentials
os.environ['KAGGLE_USERNAME'] = "sheezazahid"
os.environ['KAGGLE_KEY'] = "a00300cd353f3cb41559c53599a22955"

# Step 3: Download the IMDB dataset using Kaggle API
!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

# Step 4: Unzip the dataset
!unzip -o imdb-dataset-of-50k-movie-reviews.zip

# Step 5: Load the IMDB dataset
temp_df = pd.read_csv('/content/IMDB Dataset.csv')

# Step 6: Define a function for word-based tokenization
def tokenize_reviews(reviews):
    """
    Tokenizes each review into words.
    :param reviews: A pandas Series or list containing textual reviews.
    :return: A list of tokenized reviews (list of word tokens per review).
    """
    try:
        return [word_tokenize(review) for review in reviews]
    except Exception as e:
        print("Error during tokenization:", e)
        return None

# Step 7: Define a function to remove stop words
def remove_stopwords(tokenized_reviews):
    """
    Removes stop words from tokenized reviews.
    :param tokenized_reviews: A list of tokenized reviews (list of word tokens per review).
    :return: A list of reviews with stop words removed.
    """
    stop_words = set(stopwords.words('english'))
    try:
        return [[word for word in review if word.lower() not in stop_words] for review in tokenized_reviews]
    except Exception as e:
        print("Error during stop words removal:", e)
        return None

# Step 8: Define a function for lemmatization
def lemmatize_reviews(cleaned_reviews):
    """
    Lemmatizes each word in the cleaned reviews.
    :param cleaned_reviews: A list of reviews with stop words removed.
    :return: A list of lemmatized reviews.
    """
    lemmatizer = WordNetLemmatizer()
    try:
        return [[lemmatizer.lemmatize(word) for word in review] for review in cleaned_reviews]
    except Exception as e:
        print("Error during lemmatization:", e)
        return None

# Step 9: Apply tokenization, stop word removal, and lemmatization
try:
    # Tokenize the reviews
    temp_df['tokenized_review'] = tokenize_reviews(temp_df['review'])

    # Remove stop words
    temp_df['cleaned_review'] = remove_stopwords(temp_df['tokenized_review'])

    # Lemmatize the cleaned reviews
    temp_df['lemmatized_review'] = lemmatize_reviews(temp_df['cleaned_review'])

    # Save the updated dataset with tokenized, cleaned, and lemmatized reviews
    temp_df.to_csv('/content/IMDB_Dataset_Cleaned_Lemmatized.csv', index=False)

    # Display an example of the processed review
    print(f"Original Review: {temp_df['review'][0]}")
    print(f"Tokenized Review: {temp_df['tokenized_review'][0]}")
    print(f"Cleaned Review (No Stop Words): {temp_df['cleaned_review'][0]}")
    print(f"Lemmatized Review: {temp_df['lemmatized_review'][0]}")
    print("Lemmatized dataset saved as '/content/IMDB_Dataset_Cleaned_Lemmatized.csv'")

except Exception as e:
    print(f"An error occurred: {e}")

# Step 10: Prepare the data for machine learning
# Convert the lemmatized reviews back to text for vectorization
temp_df['lemmatized_review_text'] = temp_df['lemmatized_review'].apply(lambda x: ' '.join(x))

# Vectorization using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(temp_df['lemmatized_review_text'])

# Target variable: Sentiment (positive/negative)
y = temp_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)  # Convert labels to 0 (negative) and 1 (positive)

# Step 11: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 12: Train and evaluate models
# Logistic Regression
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

# Naive Bayes
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
y_pred_nb = nb_model.predict(X_test)

# Random Forest
rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Step 13: Evaluate models
def evaluate_model(y_test, y_pred, model_name):
    print(f"\n{model_name} Performance:")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Evaluate each model
evaluate_model(y_test, y_pred_lr, "Logistic Regression")
evaluate_model(y_test, y_pred_nb, "Naive Bayes")
evaluate_model(y_test, y_pred_rf, "Random Forest")

"""Stop words"""

#!/bin/bash
!pip install kaggle nltk pandas

import os
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Step 1: Ensure NLTK data is downloaded
try:
    nltk.download('punkt')
    nltk.download('punkt_tab')  # Added to resolve the punkt_tab error
    nltk.download('stopwords')
except Exception as e:
    print("Error downloading NLTK resources:", e)

# Step 2: Set up Kaggle API credentials
os.environ['KAGGLE_USERNAME'] = "sheezazahid"
os.environ['KAGGLE_KEY'] = "a00300cd353f3cb41559c53599a22955"

# Step 3: Download the IMDB dataset using Kaggle API
!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

# Step 4: Unzip the dataset
!unzip -o imdb-dataset-of-50k-movie-reviews.zip

# Step 5: Load the IMDB dataset
temp_df = pd.read_csv('/content/IMDB Dataset.csv')

# Step 6: Define a function for word-based tokenization
def tokenize_reviews(reviews):
    """
    Tokenizes each review into words.

    :param reviews: A pandas Series or list containing textual reviews.
    :return: A list of tokenized reviews (list of word tokens per review).
    """
    try:
        return [word_tokenize(review) for review in reviews]
    except Exception as e:
        print("Error during tokenization:", e)
        return None

# Step 7: Define a function to remove stop words
def remove_stopwords(tokenized_reviews):
    """
    Removes stop words from tokenized reviews.

    :param tokenized_reviews: A list of tokenized reviews (list of word tokens per review).
    :return: A list of reviews with stop words removed.
    """
    stop_words = set(stopwords.words('english'))
    try:
        return [[word for word in review if word.lower() not in stop_words] for review in tokenized_reviews]
    except Exception as e:
        print("Error during stop words removal:", e)
        return None

# Step 8: Apply tokenization and stop word removal
try:
    # Tokenize the reviews
    temp_df['tokenized_review'] = tokenize_reviews(temp_df['review'])

    # Remove stop words
    temp_df['cleaned_review'] = remove_stopwords(temp_df['tokenized_review'])

    # Save the updated dataset with tokenized and cleaned reviews
    temp_df.to_csv('/content/IMDB_Dataset_Cleaned.csv', index=False)

    # Display an example of the processed review
    print(f"Original Review: {temp_df['review'][0]}")
    print(f"Tokenized Review: {temp_df['tokenized_review'][0]}")
    print(f"Cleaned Review (No Stop Words): {temp_df['cleaned_review'][0]}")
    print("Cleaned dataset saved as '/content/IMDB_Dataset_Cleaned.csv'")

except Exception as e:
    print(f"An error occurred: {e}")

#!/bin/bash
!pip install kaggle nltk pandas

import os
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Step 1: Ensure NLTK data is downloaded
try:
    nltk.download('punkt')
    nltk.download('punkt_tab')  # Added to resolve the punkt_tab error
    nltk.download('stopwords')
    nltk.download('wordnet')  # Needed for lemmatization
except Exception as e:
    print("Error downloading NLTK resources:", e)

# Step 2: Set up Kaggle API credentials
os.environ['KAGGLE_USERNAME'] = "sheezazahid"
os.environ['KAGGLE_KEY'] = "a00300cd353f3cb41559c53599a22955"

# Step 3: Download the IMDB dataset using Kaggle API
!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

# Step 4: Unzip the dataset
!unzip -o imdb-dataset-of-50k-movie-reviews.zip

# Step 5: Load the IMDB dataset
temp_df = pd.read_csv('/content/IMDB Dataset.csv')

# Step 6: Define a function for word-based tokenization
def tokenize_reviews(reviews):
    """
    Tokenizes each review into words.

    :param reviews: A pandas Series or list containing textual reviews.
    :return: A list of tokenized reviews (list of word tokens per review).
    """
    try:
        return [word_tokenize(review) for review in reviews]
    except Exception as e:
        print("Error during tokenization:", e)
        return None

# Step 7: Define a function to remove stop words
def remove_stopwords(tokenized_reviews):
    """
    Removes stop words from tokenized reviews.

    :param tokenized_reviews: A list of tokenized reviews (list of word tokens per review).
    :return: A list of reviews with stop words removed.
    """
    stop_words = set(stopwords.words('english'))
    try:
        return [[word for word in review if word.lower() not in stop_words] for review in tokenized_reviews]
    except Exception as e:
        print("Error during stop words removal:", e)
        return None

# Step 8: Define a function for lemmatization
def lemmatize_reviews(cleaned_reviews):
    """
    Lemmatizes each word in the cleaned reviews.

    :param cleaned_reviews: A list of reviews with stop words removed.
    :return: A list of lemmatized reviews.
    """
    lemmatizer = WordNetLemmatizer()
    try:
        return [[lemmatizer.lemmatize(word) for word in review] for review in cleaned_reviews]
    except Exception as e:
        print("Error during lemmatization:", e)
        return None

# Step 9: Apply tokenization, stop word removal, and lemmatization
try:
    # Tokenize the reviews
    temp_df['tokenized_review'] = tokenize_reviews(temp_df['review'])

    # Remove stop words
    temp_df['cleaned_review'] = remove_stopwords(temp_df['tokenized_review'])

    # Lemmatize the cleaned reviews
    temp_df['lemmatized_review'] = lemmatize_reviews(temp_df['cleaned_review'])

    # Save the updated dataset with tokenized, cleaned, and lemmatized reviews
    temp_df.to_csv('/content/IMDB_Dataset_Cleaned_Lemmatized.csv', index=False)

    # Display an example of the processed review
    print(f"Original Review: {temp_df['review'][0]}")
    print(f"Tokenized Review: {temp_df['tokenized_review'][0]}")
    print(f"Cleaned Review (No Stop Words): {temp_df['cleaned_review'][0]}")
    print(f"Lemmatized Review: {temp_df['lemmatized_review'][0]}")
    print("Lemmatized dataset saved as '/content/IMDB_Dataset_Cleaned_Lemmatized.csv'")

except Exception as e:
    print(f"An error occurred: {e}")